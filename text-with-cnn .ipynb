{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom textblob import Word\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Flatten, Conv1D, MaxPooling1D,Bidirectional,LSTM\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D,concatenate\nfrom keras.activations import relu\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom keras.preprocessing import text, sequence\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.optimizers import Adam\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom keras.preprocessing import text, sequence\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport re\nfrom nltk.corpus import stopwords\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print(os.listdir('../input/embeddings/'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a86d1c1e34d005537bf1ef3620e990abd6bb298"},"cell_type":"code","source":"EMBEDDING_FILE = (\"../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1d577b0f6795b9fb01b4038b2bac2fcb1e24a8c"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv',encoding=\"utf-8\")\ntest = pd.read_csv('../input/test.csv',encoding='utf-8')\nsubmission = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0cbca169877e48a225b84ed2f844a1a0c2e1116"},"cell_type":"code","source":"train['question_text'][58]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2b521de84bf6277ace6a5e55078648b25f287e7"},"cell_type":"code","source":"\ntrain['question_text'] = train['question_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ntest['question_text'] = test['question_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\ndef clean_text(x):\n\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ntrain[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_text(x))\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_text(x))\n\n\n\ntrain['question_text'] = train['question_text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\ntest['question_text'] = test['question_text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n\n\ntrain['question_text'] = train['question_text'].apply(lambda x: \" \".join(x for x in x.split() if len(x) >2 ))\ntest['question_text'] = test['question_text'].apply(lambda x: \" \".join(x for x in x.split() if len(x) >2 ))\n\n\"\"\"stop = stopwords.words('english')\n\n\ntrain['question_text'] = train['question_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ntest['question_text'] = test['question_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n\ntrain['question_text'] = train['question_text'].fillna(\"\").apply(lambda x: clean_text(x))\ntest['question_text'] = test['question_text'].fillna(\"\").apply(lambda x: clean_text(x))\n\ntrain['question_text'] = train['question_text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\ntest['question_text'] = test['question_text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n\ntrain['question_text'] = train['question_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ntest['question_text'] = test['question_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n\ntrain['question_text'] = train['question_text'].apply(lambda x: \" \".join(x for x in x.split() if x.isalpha()))\ntest['question_text'] = test['question_text'].apply(lambda x: \" \".join(x for x in x.split() if x.isalpha()))\n\ntrain['question_text'] = train['question_text'].apply(lambda x: \" \".join(x for x in x.split() if len(x) >2 ))\ntest['question_text'] = test['question_text'].apply(lambda x: \" \".join(x for x in x.split() if len(x) >2 ))\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61657d488b0f9cc179fc7f6ae0421845c9166306"},"cell_type":"code","source":"train['question_text'][58]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"151c2e9fab42a03b9cecec8072dae266a7ed9670"},"cell_type":"code","source":"#train['question_text'] = train['question_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n\n#test['question_text'] = test['question_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd547b5ea3e72bb12cfbc6946144bbccd0fec559"},"cell_type":"code","source":"X_train = train['question_text']\nY_train = train['target']\nX_test = test['question_text']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"554561fd3566f3576ccc4cbd5ac0669be90f0350"},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\nsequences_train = tokenizer.texts_to_sequences(X_train)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"682b84ee7dfa336cd639c9410619e5fe7a4aa4f4"},"cell_type":"code","source":"max_input_lenght = max([len(x) for x in sequences_train])\nmax_input_lenght","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0b3c5b4b396b267840aa18c4066a85722b539e6"},"cell_type":"code","source":"x_train_data_padded = pad_sequences(sequences_train, maxlen=max_input_lenght, padding='post', truncating='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02824014c78dda6aa55480aabc403ab5bb8b6377"},"cell_type":"code","source":"x_test_data_padded = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_input_lenght, padding='post', truncating='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1aa2bba79fa7241d94169e92d38bfd6e6f0fe94b"},"cell_type":"code","source":"from keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d51596554baa257629b1b440e5ff46b20b70bbba"},"cell_type":"code","source":"word_index = tokenizer.word_index\nmax_features = len(word_index)+1\ndef load_glove(word_index):\n    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if o.split(\" \")[0] in word_index)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix \n    \ndef load_fasttext(word_index):    \n    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100 and o.split(\" \")[0] in word_index )\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n    return embedding_matrix\n\ndef load_para(word_index):\n    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100 and o.split(\" \")[0] in word_index)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n    \n    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a77ab58f191eb6929a255da15e7393f6108b7757"},"cell_type":"code","source":"import gc\n\nembedding_matrix_1 = load_glove(word_index)\nembedding_matrix_2 = load_fasttext(word_index)\n#embedding_matrix_3 = load_para(word_index)\nembedding_matrix = np.mean((embedding_matrix_1, embedding_matrix_2), axis=0)  \ndel embedding_matrix_1, embedding_matrix_2\ngc.collect()\nnp.shape(embedding_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00eb7dd15df29b18c6c4ef3e774aeb30a2a060f1"},"cell_type":"code","source":"from keras.layers import *\n\ndef squash(x, axis=-1):\n    # s_squared_norm is really small\n    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n    # return scale * x\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n    scale = K.sqrt(s_squared_norm + K.epsilon())\n    return x / scale\n\n# A Capsule Implement with Pure Keras\nclass Capsule(Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n                 activation='default', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.kernel_size = kernel_size\n        self.share_weights = share_weights\n        if activation == 'default':\n            self.activation = squash\n        else:\n            self.activation = Activation(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     # shape=self.kernel_size,\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n        for i in range(self.routings):\n            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n            c = K.softmax(b)\n            c = K.permute_dimensions(c, (0, 2, 1))\n            b = K.permute_dimensions(b, (0, 2, 1))\n            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n            if i < self.routings - 1:\n                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7d149251900ebbcb9465b8367dba61fc371c673"},"cell_type":"code","source":"embed_size=300\nfrom keras.initializers import *\n\ndef capsule():\n    K.clear_session()       \n    inp = Input(shape=(max_input_lenght,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = SpatialDropout1D(rate=0.2)(x)\n    x = Bidirectional(CuDNNGRU(100, return_sequences=True, \n                                kernel_initializer=glorot_normal(seed=12300), recurrent_initializer=orthogonal(gain=1.0, seed=10000)))(x)\n\n    x = Capsule(num_capsule=10, dim_capsule=10, routings=4, share_weights=True)(x)\n    x = Flatten()(x)\n\n    x = Dense(100, activation=\"relu\", kernel_initializer=glorot_normal(seed=12300))(x)\n    x = Dropout(0.12)(x)\n    x = BatchNormalization()(x)\n\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer=Adam(),metrics=['accuracy',f1])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e22cc91eb1ffda95c22a0005debce134b798539c"},"cell_type":"code","source":"model = capsule()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc1feddcf1bcfef14f6631d37094fa1d9c1f1c58"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(x_train_data_padded, Y_train, train_size=0.98, random_state=233)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d2d6121656533f8eee1e6e480557c441a75ec4e","scrolled":true},"cell_type":"code","source":"from keras.callbacks import *\nfilepath=\"weights.best.hdf5\"\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n#early = EarlyStopping(monitor=\"val_f1\", mode=\"max\", patience=2)\ncheckpoint = ModelCheckpoint(filepath, monitor='val_f1', verbose=2, save_best_only=True, mode='max')\nreduce_lr = ReduceLROnPlateau(monitor='val_f1', factor=0.6, patience=1, min_lr=0.0001, verbose=2)\ncallbacks_list = [annealer,reduce_lr,checkpoint]\nhistory = model.fit(x=x_train,y=y_train,batch_size=512, epochs=10,validation_split= 0.05,callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d4b3189452249ea24f6e21f7e26069e71882cac"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26ba518209bf2e538427beb00a49326623282b77"},"cell_type":"code","source":"y_prediction1 = model.predict(x=x_val,batch_size=512)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93e4e3c1a5f0ee1d7715624205e837feb0cbd7a7"},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00f9bc50deea7435b4d2053ca9e50acc476c1535"},"cell_type":"code","source":"#for i in np.arange(0.1, 1, 0.01):\n #   print(i)\n   # y_pred1 =(y_prediction1 > i).astype(int)\n    #print(classification_report(y_val, y_pred1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c88336ecfdcb4da2351f688c25402495f151c0d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17bf308ad436bd88b7c60579be9991da1d19aa57"},"cell_type":"code","source":"y_prediction = model.predict(x=x_test_data_padded, batch_size=512)\ny_pred = (y_prediction > 0.309).astype(int)\nsubmission['prediction'] = y_pred\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8aef2ed219a97b0ba26c532dff1d5efecc67366d"},"cell_type":"code","source":"#submission[submission['prediction'] == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a86de1eacd11e12feab843ee71fcd5b69faf147"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}